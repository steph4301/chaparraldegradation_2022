pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE") %>%
drop_na() %>%
filter(ISA_SCORE <= 0)%>%
arrange(by = Cluster) %>%
group_by(Cluster) %>%
summarise(sum = sum(ISA_SCORE))
isa.diff.data %>%
dplyr::select(Species, 11:18)
isa.diff.data %>%
dplyr::select(Species, 11:18) %>%
pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE")
isa.diff.sum <- isa.diff.data %>%
dplyr::select(Species, 11:18)
isa.diff.data <- isa_data %>%
mutate(k2 = `2`-`3`) %>%
mutate(k3 = `3`-`4`) %>%
mutate(k4 = `4`-`5`) %>%
mutate(k5 = `5`-`6`) %>%
mutate(k6 = `6`-`7`) %>%
mutate(k7 = `7`-`8`) %>%
mutate(k8 = `8`-`9`) %>%
mutate(k9 = `9`-`10`)
isa.diff.sum <- isa.diff.data %>%
dplyr::select(Species, 11:18)
view(isa.diff.sum)
isa.diff.data %>%
dplyr::select(Species, 11:18) %>%
pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE")
isa.diff.sum <- isa.diff.data %>%
dplyr::select(Species, 11:18) %>%
pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE")
view(isa.diff.sum)
isa.diff.sum <- isa.diff.data %>%
dplyr::select(Species, 11:18) %>%
pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE") %>%
drop_na()
isa.diff.sum <- isa.diff.data %>%
dplyr::select(Species, 11:18) %>%
pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE") %>%
drop_na() %>%
filter(ISA_SCORE <= 0) %>%
arrange(by = Cluster) %>%
group_by(Cluster) %>%
summarise(sum = sum(ISA_SCORE))
ggplot(isa.diff.sum,
aes(x = Cluster,
y = sum)) +
geom_bar(stat = "identity") +
labs(title = "", x = "Cluster", y = "Value") +
theme_minimal()
isa.diff.sum
view(isa.diff.sum)
view(isa.diff.data)
ggplot(isa.diff.sum,
aes(x = Cluster,
y = sum)) +
geom_bar(stat = "identity") +
labs(title = "", x = "Cluster", y = "Value") +
theme_minimal()
# Doing 5 clusters!
# Beta average
hcluster.abs.cover <- agnes(d, method = "gaverage", par.method = -0.25)
plot(hcluster.abs.cover)
rect.hclust(hcluster.abs.cover, k = 4)
plot(hcluster.abs.cover)
rect.hclust(hcluster.abs.cover, k = 4)
plot(hcluster.abs.cover)
plot(hcluster.abs.cover)
rect.hclust(hcluster.abs.cover, k = 5)
plot(hcluster.abs.cover)
rect.hclust(hcluster.abs.cover, k = 5)
# Cut tree into groups
sub_grp <- cutree(hcluster.abs.cover, k = 4)
# Number of members in each cluster
table(sub_grp)
hcluster.abs.cover <- agnes(d, method = "gaverage", par.method = -0.25)
plot(hcluster.abs.cover)
# Cut tree into groups
sub_grp <- cutree(hcluster.abs.cover, k = 5)
# Number of members in each cluster
table(sub_grp)
# Combine group labels into species matrix
temp.df <- abs.cover.species.matrix.df %>%
rownames_to_column()
view(abs.cover.species.matrix.df)
plot(clustering, labels = data_pc$cluster_5, main = "Hierarchical Clustering Dendrogram") # these seem to be the major splits
# Step 4: Plot the dendrogram
plot(clustering, labels = rownames(data), main = "Hierarchical Clustering Dendrogram")
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
view(df)
# Number of members in each cluster
table(sub_grp)
view(sub_grp)
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
# Doing 5 clusters!
# Beta average
hcluster.abs.cover <- agnes(d, method = "gaverage", par.method = -0.25)
plot(hcluster.abs.cover)
# NAME data set
df <- data_pc %>%
dplyr::select(!c(site, site_rep))
d <- vegdist(df, method = "bray")  # Bray-Curtis dissimilarity
view(d)
# Step 3: Perform hierarchical clustering
clustering <- hclust(diss_matrix, method = "bray")  # Ward's method
# Step 3: Perform hierarchical clustering
clustering <- hclust(df, method = "bray")  # Ward's method
ISA.abs.cover <- multipatt(x = df, cluster = sub_grp, duleg = FALSE)
# A = The values are the **specificity** component of IV for that species in that combination of groups.
# B = The values are the **fidelity** component of IV for that species in that combination of groups
summary(ISA.abs.cover, indvalcomp=TRUE)
# Step 3: Perform hierarchical clustering
clustering <- hclust(diss_matrix, method = "ward.D2")  # Ward's method
# Step 4: Plot the dendrogram
plot(clustering, labels = rownames(data), main = "Hierarchical Clustering Dendrogram")
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
# Step 3: Perform hierarchical clustering
clustering <- hclust(diss_matrix, method = "gaverage")  # Ward's method
# Step 3: Perform hierarchical clustering
clustering <- hclust(diss_matrix, method = "ward.D2")  # Ward's method
clustering
clustering
# Step 3: Perform hierarchical clustering
clustering <- hclust(diss_matrix, method = "gaverage")  # Ward's method
# Step 3: Perform hierarchical clustering
clustering <- hclust(diss_matrix, method = "gaverage", par.method = -0.25)  # Ward's method
# NAME data set
df <- data_pc %>%
dplyr::select(!c(site, site_rep))
d <- vegdist(df, method = "bray")  # Bray-Curtis dissimilarity
# Step 3: Perform hierarchical clustering
clustering <- agnes(d, method = "gaverage", par.method = -0.25)
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
cluster_5 <- cutree(clustering, k = 5)  # Cut the tree into 4 clusters
table(cluster_5)
data_pc$cluster_5 <- cluster_5
plot(clustering, labels = data_pc$cluster_5, main = "Hierarchical Clustering Dendrogram") # these seem to be the major splits
isa_results_5 <- multipatt(data, cluster_5, control = how(nperm=999)) # <- 5 clusters
summary(isa_results_5)
species_pvalues_5 <- isa_results_5$sign
significant_species_5 <- species_pvalues_5 %>%
filter(p.value <= 0.05)
significant_species_5
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
significant_species_5
summary(isa_results_5)
# NAME data set
df <- data_pc %>%
dplyr::select(!c(site, site_rep))
view(df)
d <- vegdist(df, method = "bray")  # Bray-Curtis dissimilarity
# Step 3: Perform hierarchical clustering
clustering <- agnes(d, method = "gaverage", par.method = -0.25)
# Step 4: Plot the dendrogram
# plot(clustering, labels = rownames(data), main = "Hierarchical Clustering Dendrogram")
plot(clustering, labels = data_pc$site_rep, main = "Hierarchical Clustering Dendrogram")
# Doing 5 clusters!
# Beta average
hcluster.abs.cover <- agnes(d, method = "gaverage", par.method = -0.25)
# Cut tree into groups
sub_grp <- cutree(hcluster.abs.cover, k = 5)
# Number of members in each cluster
table(sub_grp)
view(data_pc)
# Combine group labels into species matrix
temp.df <- data_pc %>%
rownames_to_column()
view(temp.df)
my_groups <- as.data.frame(sub_grp) %>%
rownames_to_column()
view(my_groups)
# Combine group labels into species matrix
temp.df <- data_pc %>%
rownames_to_column()
cluster.subgrp.df <- left_join(temp.df, my_groups, by = "rowname")
view(cluster.subgrp.df)
ISA.abs.cover <- multipatt(x = df, cluster = sub_grp, duleg = FALSE)
# A = The values are the **specificity** component of IV for that species in that combination of groups.
# B = The values are the **fidelity** component of IV for that species in that combination of groups
summary(ISA.abs.cover, indvalcomp=TRUE)
# NAME data set
df <- data_pc %>%
dplyr::select(!c(site, site_rep))
d <- vegdist(df, method = "bray")  # Bray-Curtis dissimilarity
# Calculate distance measure matrix, Bray is the best for community data
# Jaccard Index and Sorensen-Dice Coefficient treat percent cover data as binary (present/absent)
# Check which method produces best clustering
# average = Computes the average distance between all pairs of observations across two clusters.
# single = Uses the minimum distance between pairs of observations in two clusters.
# complete = Considers the maximum distance between pairs of observations in two clusters.
# ward = Minimizes the total within-cluster variance; merges clusters with the smallest increase in total variance.
# gaverage = Merges clusters based on the distance between their centroids (means of points in clusters).
m <- c( "average", "single", "complete", "ward", "gaverage")
names(m) <- c( "average", "single", "complete", "ward", "gaverage")
# function to compute coefficient
ac <- function(x) {
agnes(diss_matrix, method = x, diss = TRUE, par.method = -0.25)$ac}
# Best method?
map_dbl(m, ac)
# Best method is g average at 0.8563148 = Flexible Beta method
#########################
#### Find optimal number of clusters
# Scree plot
fviz_nbclust(df, FUN = hcut, diss = d, method = "wss")
# Average Silhouette
# Measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The Average Silhouette Width is the mean of all silhouette scores within a cluster or across all clusters.
# S â‰ˆ 1: The point is well clustered
fviz_nbclust(df, FUN = hcut, diss = d, method = "silhouette")
# 4 groups has highest Avg. sillhouete width
# Gap Stat
# Compares the total within-cluster variation for different numbers of clusters with their expected values under a null reference distribution of the data (i.e., uniformly distributed points).
# The optimal number of clusters is the smallest k such that the Gap Statistic is maximized or the first local maximum occurs, indicating that adding more clusters does not significantly improve the clustering.
fviz_nbclust(df, FUN = hcut, diss = d, method = "gap_stat")
# Eight clusters is best, adding more clusteres doesn't improve clustering
# My conclusion is 5 clusters is optimum
# Above tests suggest at least two, but are also not conclusive...
# Define function to compute Mantel correlation tests for different cluster assignments
compute_mantel_correlation <- function(hclust_obj, distance_matrix) {
# Initialize variables to store results
mantel_correlation <- numeric(length = length(hclust_obj$order) - 1)
# Iterate through different numbers of clusters
for (k in 2:length(hclust_obj$order)) {
cluster_assignments <- cutree(hclust_obj, k = k)
cluster_distance <- as.matrix(dist(cluster_assignments))
mantel_result <- mantel(cluster_distance, distance_matrix)
mantel_correlation[k - 1] <- mantel_result$statistic
# Print Mantel correlation for each number of clusters
print(paste("Mantel Correlation (r) for", k, "clusters:", mantel_result$statistic))
}
return(mantel_correlation)
}
# Create Cluster and then run tests
Hclust.result <- agnes(d, method = "gaverage", par.method = -0.25)
mantel_results <- round(compute_mantel_correlation(Hclust.result, d), 3)
# Find the optimal number of clusters based on the largest Mantel correlation, r
optimal_clusters <- which.max(mantel_results) + 1  # Add 1 to convert index to number of clusters
print(paste("Optimal Number of Clusters:", optimal_clusters))
# 4 groups = 0.608
# 6 groups = 0.560
# 5 groups = 0.554
# 7 groups = 0.520
####
#### Use ISA to determine optimal clusters
# Function to run multipatt() on k iterations of clusters and extract ISA values
run_multipatt_and_extract_ISA <- function(data, max_clusters) {
# Initialize an empty dataframe to store ISA values
isa_df <- data.frame(Species = colnames(data))
# Iterate through different numbers of clusters
for (k in 2:max_clusters) {
cluster_assignments <- cutree(agnes(vegdist(data, method = "bray"), method = "gaverage", par.method = -0.25), k = k)
# Convert cluster assignments to a factor
cluster_factor <- as.factor(cluster_assignments)
# Run multipatt() with the cluster factor
multipatt_result <- multipatt(data, cluster = cluster_factor, duleg = FALSE)
# Extract ISA values and p-values
isa_values <- multipatt_result$sign$index
p_values <- multipatt_result$sign$p.value
# Filter by p-value <= 0.05
significant_isa_values <- ifelse(p_values <= 0.05, isa_values, NA)
# Extract ISA values for each species and add to the dataframe
isa_df[[as.character(k)]] <- significant_isa_values
}
return(isa_df)
}
max_clusters <- 10  # Maximum number of clusters to iterate through
# Run multipatt() and extract ISA values for each species
isa_data <- run_multipatt_and_extract_ISA(df, max_clusters)
# Frequency distribution of number of significant ISA in each cluster step
frq.ISA <- isa_data %>%
pivot_longer(cols = 2:10, names_to = "Cluster", values_to = "ISA_SCORE") %>%
drop_na() %>%
group_by(Cluster) %>%
summarise(count = n()) %>%
arrange(as.numeric(Cluster))
#######
# Now calculate the sum of the differences in ISA scores from one step to the next
# When the indicator values of all species are decreasing, the clustering method does not explain anything more.
# Take difference of species index values
isa.diff.data <- isa_data %>%
mutate(k2 = `2`-`3`) %>%
mutate(k3 = `3`-`4`) %>%
mutate(k4 = `4`-`5`) %>%
mutate(k5 = `5`-`6`) %>%
mutate(k6 = `6`-`7`) %>%
mutate(k7 = `7`-`8`) %>%
mutate(k8 = `8`-`9`) %>%
mutate(k9 = `9`-`10`)
isa.diff.sum <- isa.diff.data %>%
dplyr::select(Species, 11:18) %>%
pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE") %>%
drop_na() %>%
filter(ISA_SCORE <= 0) %>%
arrange(by = Cluster) %>%
group_by(Cluster) %>%
summarise(sum = sum(ISA_SCORE))
ggplot(isa.diff.sum,
aes(x = Cluster,
y = sum)) +
geom_bar(stat = "identity") +
labs(title = "", x = "Cluster", y = "Value") +
theme_minimal()
# The differences from 2 to 3 groups (-8) and from 3 to 4 groups (-4) are small,
# indicating that moving from 2 to 4 clusters doesnâ€™t significantly improve the ISA scores.
# This suggests that the structure in the data is relatively stable with these numbers of clusters,
# and adding more groups is not greatly improving how well species are associated with their clusters.
# The difference from 4 to 5 groups is -90, which is a noticeable drop. This indicates that introducing a 5th cluster starts to capture more distinct ecological patterns, improving the ISA scores.
# Optimal Number of Clusters:
# 4 clusters may be suboptimal due to the small improvement seen moving to 4 clusters from 3 (-4).
# 5 clusters is likely the best choice. The drop to -90 suggests that the 5-cluster solution
# is where distinct ecological patterns are maximized before significant over-segmentation occurs.
# Adding more than 5 clusters likely results in diminishing returns, as indicated by the sharp declines in ISA scores.
# Doing 5 clusters!
# Beta average
hcluster.abs.cover <- agnes(d, method = "gaverage", par.method = -0.25)
# Negative values of beta (e.g., -0.25) make the clustering process more similar to single linkage (minimum distance between clusters).
# Positive values of beta make the process more similar to complete linkage (maximum distance). beta = 0 corresponds to the standard average linkage (UPGMA).
plot(hcluster.abs.cover)
rect.hclust(hcluster.abs.cover, k = 5)
# Cut tree into groups
sub_grp <- cutree(hcluster.abs.cover, k = 5)
# Number of members in each cluster
table(sub_grp)
# Combine group labels into species matrix
temp.df <- data_pc %>%
rownames_to_column()
my_groups <- as.data.frame(sub_grp) %>%
rownames_to_column()
cluster.subgrp.df <- left_join(temp.df, my_groups, by = "rowname")
################
# Indicator Species Analysis
ISA.abs.cover <- multipatt(x = df, cluster = sub_grp, duleg = FALSE)
# A = The values are the **specificity** component of IV for that species in that combination of groups.
# B = The values are the **fidelity** component of IV for that species in that combination of groups
summary(ISA.abs.cover, indvalcomp=TRUE)
# NAME data set
df <- data_pc %>%
dplyr::select(!c(site, site_rep))
d <- vegdist(df, method = "bray")  # Bray-Curtis dissimilarity
# Calculate distance measure matrix, Bray is the best for community data
# Jaccard Index and Sorensen-Dice Coefficient treat percent cover data as binary (present/absent)
# Check which method produces best clustering
# average = Computes the average distance between all pairs of observations across two clusters.
# single = Uses the minimum distance between pairs of observations in two clusters.
# complete = Considers the maximum distance between pairs of observations in two clusters.
# ward = Minimizes the total within-cluster variance; merges clusters with the smallest increase in total variance.
# gaverage = Merges clusters based on the distance between their centroids (means of points in clusters).
m <- c( "average", "single", "complete", "ward", "gaverage")
names(m) <- c( "average", "single", "complete", "ward", "gaverage")
# function to compute coefficient
ac <- function(x) {
agnes(diss_matrix, method = x, diss = TRUE, par.method = -0.25)$ac}
# Best method?
map_dbl(m, ac)
# Best method is g average at 0.8563148 = Flexible Beta method
#########################
#### Find optimal number of clusters
# Scree plot
fviz_nbclust(df, FUN = hcut, diss = d, method = "wss")
# Average Silhouette
# Measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The Average Silhouette Width is the mean of all silhouette scores within a cluster or across all clusters.
# S â‰ˆ 1: The point is well clustered
fviz_nbclust(df, FUN = hcut, diss = d, method = "silhouette")
# 4 groups has highest Avg. sillhouete width
# Gap Stat
# Compares the total within-cluster variation for different numbers of clusters with their expected values under a null reference distribution of the data (i.e., uniformly distributed points).
# The optimal number of clusters is the smallest k such that the Gap Statistic is maximized or the first local maximum occurs, indicating that adding more clusters does not significantly improve the clustering.
fviz_nbclust(df, FUN = hcut, diss = d, method = "gap_stat")
# Eight clusters is best, adding more clusteres doesn't improve clustering
# My conclusion is 5 clusters is optimum
# Above tests suggest at least two, but are also not conclusive...
# Define function to compute Mantel correlation tests for different cluster assignments
compute_mantel_correlation <- function(hclust_obj, distance_matrix) {
# Initialize variables to store results
mantel_correlation <- numeric(length = length(hclust_obj$order) - 1)
# Iterate through different numbers of clusters
for (k in 2:length(hclust_obj$order)) {
cluster_assignments <- cutree(hclust_obj, k = k)
cluster_distance <- as.matrix(dist(cluster_assignments))
mantel_result <- mantel(cluster_distance, distance_matrix)
mantel_correlation[k - 1] <- mantel_result$statistic
# Print Mantel correlation for each number of clusters
print(paste("Mantel Correlation (r) for", k, "clusters:", mantel_result$statistic))
}
return(mantel_correlation)
}
# Create Cluster and then run tests
Hclust.result <- agnes(d, method = "gaverage", par.method = -0.25)
mantel_results <- round(compute_mantel_correlation(Hclust.result, d), 3)
# Find the optimal number of clusters based on the largest Mantel correlation, r
optimal_clusters <- which.max(mantel_results) + 1  # Add 1 to convert index to number of clusters
print(paste("Optimal Number of Clusters:", optimal_clusters))
# 4 groups = 0.608
# 6 groups = 0.560
# 5 groups = 0.554
# 7 groups = 0.520
####
#### Use ISA to determine optimal clusters
# Function to run multipatt() on k iterations of clusters and extract ISA values
run_multipatt_and_extract_ISA <- function(data, max_clusters) {
# Initialize an empty dataframe to store ISA values
isa_df <- data.frame(Species = colnames(data))
# Iterate through different numbers of clusters
for (k in 2:max_clusters) {
cluster_assignments <- cutree(agnes(vegdist(data, method = "bray"), method = "gaverage", par.method = -0.25), k = k)
# Convert cluster assignments to a factor
cluster_factor <- as.factor(cluster_assignments)
# Run multipatt() with the cluster factor
multipatt_result <- multipatt(data, cluster = cluster_factor, duleg = FALSE)
# Extract ISA values and p-values
isa_values <- multipatt_result$sign$index
p_values <- multipatt_result$sign$p.value
# Filter by p-value <= 0.05
significant_isa_values <- ifelse(p_values <= 0.05, isa_values, NA)
# Extract ISA values for each species and add to the dataframe
isa_df[[as.character(k)]] <- significant_isa_values
}
return(isa_df)
}
max_clusters <- 10  # Maximum number of clusters to iterate through
# Run multipatt() and extract ISA values for each species
isa_data <- run_multipatt_and_extract_ISA(df, max_clusters)
# Frequency distribution of number of significant ISA in each cluster step
frq.ISA <- isa_data %>%
pivot_longer(cols = 2:10, names_to = "Cluster", values_to = "ISA_SCORE") %>%
drop_na() %>%
group_by(Cluster) %>%
summarise(count = n()) %>%
arrange(as.numeric(Cluster))
#######
# Now calculate the sum of the differences in ISA scores from one step to the next
# When the indicator values of all species are decreasing, the clustering method does not explain anything more.
# Take difference of species index values
isa.diff.data <- isa_data %>%
mutate(k2 = `2`-`3`) %>%
mutate(k3 = `3`-`4`) %>%
mutate(k4 = `4`-`5`) %>%
mutate(k5 = `5`-`6`) %>%
mutate(k6 = `6`-`7`) %>%
mutate(k7 = `7`-`8`) %>%
mutate(k8 = `8`-`9`) %>%
mutate(k9 = `9`-`10`)
isa.diff.sum <- isa.diff.data %>%
dplyr::select(Species, 11:18) %>%
pivot_longer(cols = 2:9, names_to = "Cluster", values_to = "ISA_SCORE") %>%
drop_na() %>%
filter(ISA_SCORE <= 0) %>%
arrange(by = Cluster) %>%
group_by(Cluster) %>%
summarise(sum = sum(ISA_SCORE))
ggplot(isa.diff.sum,
aes(x = Cluster,
y = sum)) +
geom_bar(stat = "identity") +
labs(title = "", x = "Cluster", y = "Value") +
theme_minimal()
# The differences from 2 to 3 groups (-8) and from 3 to 4 groups (-4) are small,
# indicating that moving from 2 to 4 clusters doesnâ€™t significantly improve the ISA scores.
# This suggests that the structure in the data is relatively stable with these numbers of clusters,
# and adding more groups is not greatly improving how well species are associated with their clusters.
# The difference from 4 to 5 groups is -90, which is a noticeable drop. This indicates that introducing a 5th cluster starts to capture more distinct ecological patterns, improving the ISA scores.
# Optimal Number of Clusters:
# 4 clusters may be suboptimal due to the small improvement seen moving to 4 clusters from 3 (-4).
# 5 clusters is likely the best choice. The drop to -90 suggests that the 5-cluster solution
# is where distinct ecological patterns are maximized before significant over-segmentation occurs.
# Adding more than 5 clusters likely results in diminishing returns, as indicated by the sharp declines in ISA scores.
# Doing 5 clusters!
# Beta average
hcluster.abs.cover <- agnes(d, method = "gaverage", par.method = -0.25)
# Negative values of beta (e.g., -0.25) make the clustering process more similar to single linkage (minimum distance between clusters).
# Positive values of beta make the process more similar to complete linkage (maximum distance). beta = 0 corresponds to the standard average linkage (UPGMA).
plot(hcluster.abs.cover)
rect.hclust(hcluster.abs.cover, k = 5)
# Cut tree into groups
sub_grp <- cutree(hcluster.abs.cover, k = 5)
# Number of members in each cluster
table(sub_grp)
# Combine group labels into species matrix
temp.df <- data_pc %>%
rownames_to_column()
my_groups <- as.data.frame(sub_grp) %>%
rownames_to_column()
cluster.subgrp.df <- left_join(temp.df, my_groups, by = "rowname")
################
# Indicator Species Analysis
ISA.abs.cover <- multipatt(x = df, cluster = sub_grp, duleg = FALSE)
# A = The values are the **specificity** component of IV for that species in that combination of groups.
# B = The values are the **fidelity** component of IV for that species in that combination of groups
summary(ISA.abs.cover, indvalcomp=TRUE)
# Check for homogeneity of dispersion among groups, required for MANOVA
trt <- as.character(my_groups$sub_grp)
# There is significant dispersion within groups, and groups are unbalanced --- cannot use MANOVA (adonis)
anova(betadisper(d, trt))
mod <- betadisper(d, trt)
TukeyHSD(mod)
boxplot(mod)
plot(mod)
# Best option is MRPP
# The key idea is simple: if groups differ, the mean within-group dissimilarity should be smaller
# than the mean dissimilarity among randomly selected groups of the same size.
# Sort of low effect size but groups are meaningful!
results <- mrpp(d, grouping = sub_grp, permutations = 999)
results
results
results
# Best option is MRPP
# The key idea is simple: if groups differ, the mean within-group dissimilarity should be smaller
# than the mean dissimilarity among randomly selected groups of the same size.
# Sort of low effect size but groups are meaningful!
results <- mrpp(d, grouping = sub_grp, permutations = 999)
results
